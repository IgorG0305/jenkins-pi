# ===============================
# üì¶ Importa√ß√£o das Depend√™ncias
# ===============================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.linear_model import Ridge # Usando Ridge como no original
from sklearn.metrics import (
    classification_report, ConfusionMatrixDisplay, silhouette_score,
    r2_score, mean_squared_error, confusion_matrix
)
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import nltk
from nltk.corpus import stopwords
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import RandomForestClassifier as SparkRF # Renomeado para evitar conflito
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from sqlalchemy import create_engine
import warnings

# Ignorar warnings para limpeza da sa√≠da
warnings.filterwarnings('ignore')

# Tentar baixar stopwords (j√° feito no Dockerfile, mas garante)
try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')

# ================================
# ‚öôÔ∏è Configura√ß√£o Inicial
# ================================
# Configura√ß√£o da conex√£o (Usada para Pandas e talvez Spark)
usuario = 'dbpi'
senha = 'walker1207'
host = 'db' # <-- MUITO IMPORTANTE: Usar o nome do servi√ßo Docker
porta = 3306
nome_banco_1 = 'faculdades1'
nome_banco_2 = 'faculdade' # <-- ATEN√á√ÉO: √â outra base?

# String de conex√£o 1
try:
    engine1 = create_engine(f'mysql+pymysql://{usuario}:{senha}@{host}:{porta}/{nome_banco_1}')
    print("Conex√£o com 'faculdades1' estabelecida (Pandas).")
except Exception as e:
    print(f"Erro ao conectar a 'faculdades1': {e}")
    exit() # Se n√£o conectar, n√£o adianta continuar

# String de conex√£o 2 (Ajustado para 'db', mas verifique se √© 'faculdade' ou 'faculdades1')
try:
    # ATEN√á√ÉO: Verifique se as credenciais (root sem senha) e o banco 'faculdade' existem no container 'db'
    engine2 = create_engine(f'mysql+pymysql://root:@{host}:{porta}/{nome_banco_2}') # <-- Ajustado para 'db', mas verifique as credenciais!
    print("Conex√£o com 'faculdade' estabelecida (Pandas).")
except Exception as e:
    print(f"Erro ao conectar a 'faculdade': {e}")
    # Decida se deve sair ou continuar se esta conex√£o falhar
    # exit()


# ================================
# üì• Leitura e An√°lise Inicial (Pandas)
# ================================
print("\n--- Lendo dados de 'alunos_tratados' ---")
try:
    df = pd.read_sql("SELECT * FROM alunos_tratados", con=engine1)
    print("Dados lidos com sucesso.")
    print(df.head())
    print(df.info())
    print("\nValores Nulos:")
    print(df.isnull().sum())
except Exception as e:
    print(f"Erro ao ler 'alunos_tratados': {e}")
    exit()

# ===============================
# üîß Iniciar sess√£o Spark
# ===============================
print("\n--- Iniciando Sess√£o Spark ---")
spark = SparkSession.builder \
    .appName("PrevisaoRiscoEvasao") \
    .getOrCreate() # <-- JAR e HDFS removidos, ser√£o gerenciados externamente/n√£o usados

print("Sess√£o Spark iniciada.")

# ===============================
# üì• Leitura dos dados via JDBC (Spark)
# ===============================
print("\n--- Lendo dados via JDBC (Spark) ---")
try:
    df_spark = spark.read.format("jdbc").options(
        url=f"jdbc:mysql://{host}:{porta}/{nome_banco_1}", # <-- Usando 'db'
        driver="com.mysql.cj.jdbc.Driver",
        dbtable="alunos_tratados",
        user=usuario,
        password=senha
    ).load()

    print("Dados lidos com sucesso pelo Spark.")
    df_spark.printSchema()
    df_spark.show(5)
except Exception as e:
    print(f"Erro ao ler dados com Spark JDBC: {e}")
    spark.stop()
    exit()

# ===============================
# üßº Limpeza e convers√£o (Spark)
# ===============================
print("\n--- Processando dados no Spark ---")
colunas_numericas = ["idade", "renda_familiar", "faltas_1", "notas_1", "faltas_2", "notas_2",
                     "faltas_3", "notas_3", "faltas_4", "notas_4", "faltas_5", "notas_5"]
colunas_categoricas = ["sexo", "trabalha", "acompanhamento_medico", "tem_filho",
                       "estado_civil", "desempenho_1", "desempenho_2",
                       "desempenho_3", "desempenho_4", "desempenho_5", "risco_evasao"]

# Garante que as colunas existem antes de processar
df_spark_cols = df_spark.columns

for coln in colunas_numericas:
    if coln in df_spark_cols:
        df_spark = df_spark.withColumn(coln, col(coln).cast("double"))
        df_spark = df_spark.na.fill({coln: -1.0}) # Preencher com float se a coluna √© double
    else:
        print(f"Aviso: Coluna num√©rica '{coln}' n√£o encontrada no DataFrame Spark.")


for colc in colunas_categoricas:
    if colc in df_spark_cols:
        df_spark = df_spark.na.fill({colc: "desconhecido"})
        indexer = StringIndexer(inputCol=colc, outputCol=colc + "_idx", handleInvalid="keep")
        df_spark = indexer.fit(df_spark).transform(df_spark)
    else:
         print(f"Aviso: Coluna categ√≥rica '{colc}' n√£o encontrada no DataFrame Spark.")

# ===============================
# üîÑ Vetoriza√ß√£o (Spark)
# ===============================
# Apenas colunas que realmente existem e foram indexadas/convertidas
feature_cols_spark = [c for c in colunas_numericas if c in df_spark_cols] + \
                     [c + "_idx" for c in colunas_categoricas[:-1] if c + "_idx" in df_spark.columns]

print(f"\nFeatures usadas no Spark: {feature_cols_spark}")

if "risco_evasao_idx" not in df_spark.columns:
    print("Erro: Coluna target 'risco_evasao_idx' n√£o foi criada. Verifique os dados de entrada.")
    spark.stop()
    exit()

assembler = VectorAssembler(inputCols=feature_cols_spark, outputCol="features_vec")
df_spark = assembler.transform(df_spark)

# ===============================
# üå≥ Random Forest Classifier (Spark)
# ===============================
print("\n--- Treinando Modelo Spark (Random Forest) ---")
rf_spark = SparkRF(labelCol="risco_evasao_idx", featuresCol="features_vec", numTrees=100)
model_spark = rf_spark.fit(df_spark)
predictions = model_spark.transform(df_spark)

# ===============================
# üìä Avalia√ß√£o (Spark)
# ===============================
print("\n--- Avaliando Modelo Spark ---")
evaluator = MulticlassClassificationEvaluator(
    labelCol="risco_evasao_idx", predictionCol="prediction", metricName="accuracy"
)
accuracy = evaluator.evaluate(predictions)
print(f"Acur√°cia do modelo Spark: {accuracy:.4f}")

# Obter Matriz de Confus√£o (Coletando dados - CUIDADO com datasets grandes)
print("\n--- Gerando Matriz de Confus√£o (Spark -> Pandas) ---")
preds_pd = predictions.select("risco_evasao_idx", "prediction").toPandas()
conf_mat = confusion_matrix(preds_pd["risco_evasao_idx"], preds_pd["prediction"])
print("Matriz de Confus√£o:")
print(conf_mat)

# ===============================
# üìà Visualiza√ß√µes (Comentadas para Jenkins)
# ===============================
# A exibi√ß√£o interativa n√£o funciona no Jenkins. Salve em arquivos se necess√°rio.

print("\n--- Gera√ß√£o de gr√°ficos (comentada/adaptada para salvar) ---")

# --- Exemplo: Salvando um gr√°fico Matplotlib ---
# plt.figure(figsize=(8, 6))
# sns.heatmap(conf_mat, annot=True, fmt="d", cmap="Blues")
# plt.title("Matriz de Confus√£o - Spark")
# plt.xlabel("Predito")
# plt.ylabel("Real")
# plt.savefig("/opt/bitnami/spark/scripts/matriz_confusao_spark.png") # Salva em um arquivo
# plt.close() # Fecha a figura para liberar mem√≥ria
# print("Matriz de confus√£o salva em /opt/bitnami/spark/scripts/matriz_confusao_spark.png")

# --- Exemplo: Salvando um gr√°fico Plotly (requer 'kaleido') ---
# Para salvar Plotly, instale: pip install kaleido
# df_plot = pd.DataFrame({'PCA1': ..., 'PCA2': ..., 'Cluster': ...})
# fig = px.scatter(df_plot, x='PCA1', y='PCA2', color='Cluster')
# fig.write_image("/opt/bitnami/spark/scripts/cluster_pca.png")
# print("Gr√°fico de cluster salvo em /opt/bitnami/spark/scripts/cluster_pca.png")

# ===============================
# üõë Parar Sess√£o Spark
# ===============================
print("\n--- Parando Sess√£o Spark ---")
spark.stop()
print("Sess√£o Spark finalizada.")
print("\n--- Script Python Conclu√≠do ---")